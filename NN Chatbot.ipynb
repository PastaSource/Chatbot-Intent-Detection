{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3bee75-b7f4-45f5-b1d1-c9ab4c1e31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install numpy==1.22.4\n",
    "# !pip install requests==2.27.1\n",
    "# !pip install setuptools==62.3.2\n",
    "# !pip install scikit-learn\n",
    "# !pip install openpyxl\n",
    "# !pip install tensorflow\n",
    "# !pip install --ignore-installed --upgrade --user tensorflow-gpu\n",
    "# !pip install tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd20818-ef71-4826-93e7-06ac3ccde54d",
   "metadata": {},
   "source": [
    "### Check Tensorflow is running in GPU mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d32b10-3946-4a1b-9ad2-28813732386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "len(tf.config.list_physical_devices('GPU'))>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20abae0c-0a86-4cda-aaab-bffb05365dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from nltk.stem import wordnet # Lemmitization\n",
    "from nltk import pos_tag # Part Of Speech\n",
    "from nltk import word_tokenize # Tokenize text\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Bow\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # TFIDF\n",
    "from sklearn.metrics import pairwise_distances # Cosine similarity\n",
    "\n",
    "# NN Dependencies\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a342c23-7753-4566-9b3a-13b5f6c90178",
   "metadata": {},
   "source": [
    "### Load our data in a pandas instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55104f37-b072-4fa2-bbe8-42aba7f22fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Text Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me about your personality</td>\n",
       "      <td>Just think of me as the ace up your sleeve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want to know you better</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Define yourself</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe yourself</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell me about yourself</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all about you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tell me some stuff about you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>talk some stuff about you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>talk about yourself</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>about yourself</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>who are you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>introduce yourself</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I want to know more about you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>what are you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>what is your personality</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>say about you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tell me about you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>why are you here</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>are you 21 years old</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>what is your age</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>how old are you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>age of yours</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>how old is your platform</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tell me your age</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I'd like to know your age</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>you are annoying</td>\n",
       "      <td>Sorry to come across that way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I find you annoying</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>you're incredibly annoying</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>you're so annoying</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>you're too annoying</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>you are annoying me so much</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>you annoy me</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>you are such annoying</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Context  \\\n",
       "0   Tell me about your personality   \n",
       "1        I want to know you better   \n",
       "2                  Define yourself   \n",
       "3                Describe yourself   \n",
       "4           tell me about yourself   \n",
       "5                    all about you   \n",
       "6     tell me some stuff about you   \n",
       "7        talk some stuff about you   \n",
       "8              talk about yourself   \n",
       "9                   about yourself   \n",
       "10                     who are you   \n",
       "11              introduce yourself   \n",
       "12   I want to know more about you   \n",
       "13                    what are you   \n",
       "14        what is your personality   \n",
       "15                   say about you   \n",
       "16               tell me about you   \n",
       "17                why are you here   \n",
       "18                             NaN   \n",
       "19            are you 21 years old   \n",
       "20                what is your age   \n",
       "21                 how old are you   \n",
       "22                    age of yours   \n",
       "23        how old is your platform   \n",
       "24                tell me your age   \n",
       "25       I'd like to know your age   \n",
       "26                             NaN   \n",
       "27                you are annoying   \n",
       "28             I find you annoying   \n",
       "29      you're incredibly annoying   \n",
       "30              you're so annoying   \n",
       "31             you're too annoying   \n",
       "32     you are annoying me so much   \n",
       "33                    you annoy me   \n",
       "34           you are such annoying   \n",
       "\n",
       "                                        Text Response  \n",
       "0         Just think of me as the ace up your sleeve.  \n",
       "1       I can help you work smarter instead of harder  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14                                                NaN  \n",
       "15                                                NaN  \n",
       "16                                                NaN  \n",
       "17                                                NaN  \n",
       "18                                                NaN  \n",
       "19  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "20                                                NaN  \n",
       "21                                                NaN  \n",
       "22                                                NaN  \n",
       "23                                                NaN  \n",
       "24                                                NaN  \n",
       "25                                                NaN  \n",
       "26                                                NaN  \n",
       "27                     Sorry to come across that way.  \n",
       "28                         Sorry to make you feel so.  \n",
       "29                                                NaN  \n",
       "30                                                NaN  \n",
       "31                                                NaN  \n",
       "32                                                NaN  \n",
       "33                                                NaN  \n",
       "34                                                NaN  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data\\\\dialog_talk_agent.xlsx')\n",
    "df.head(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceab052-5b65-4e9b-9e83-01789e275d2a",
   "metadata": {},
   "source": [
    "### Fills null values with previous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2aa0da4f-33a4-416d-a879-e2729be0ccf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Text Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me about your personality</td>\n",
       "      <td>Just think of me as the ace up your sleeve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want to know you better</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Define yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell me about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tell me some stuff about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>talk some stuff about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>talk about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>who are you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>introduce yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I want to know more about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>what are you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>what is your personality</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>say about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tell me about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>why are you here</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>why are you here</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>are you 21 years old</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>what is your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>how old are you</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>age of yours</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>how old is your platform</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tell me your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I'd like to know your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I'd like to know your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>you are annoying</td>\n",
       "      <td>Sorry to come across that way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I find you annoying</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>you're incredibly annoying</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>you're so annoying</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>you're too annoying</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>you are annoying me so much</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>you annoy me</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>you are such annoying</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Context  \\\n",
       "0   Tell me about your personality   \n",
       "1        I want to know you better   \n",
       "2                  Define yourself   \n",
       "3                Describe yourself   \n",
       "4           tell me about yourself   \n",
       "5                    all about you   \n",
       "6     tell me some stuff about you   \n",
       "7        talk some stuff about you   \n",
       "8              talk about yourself   \n",
       "9                   about yourself   \n",
       "10                     who are you   \n",
       "11              introduce yourself   \n",
       "12   I want to know more about you   \n",
       "13                    what are you   \n",
       "14        what is your personality   \n",
       "15                   say about you   \n",
       "16               tell me about you   \n",
       "17                why are you here   \n",
       "18                why are you here   \n",
       "19            are you 21 years old   \n",
       "20                what is your age   \n",
       "21                 how old are you   \n",
       "22                    age of yours   \n",
       "23        how old is your platform   \n",
       "24                tell me your age   \n",
       "25       I'd like to know your age   \n",
       "26       I'd like to know your age   \n",
       "27                you are annoying   \n",
       "28             I find you annoying   \n",
       "29      you're incredibly annoying   \n",
       "30              you're so annoying   \n",
       "31             you're too annoying   \n",
       "32     you are annoying me so much   \n",
       "33                    you annoy me   \n",
       "34           you are such annoying   \n",
       "\n",
       "                                        Text Response  \n",
       "0         Just think of me as the ace up your sleeve.  \n",
       "1       I can help you work smarter instead of harder  \n",
       "2       I can help you work smarter instead of harder  \n",
       "3       I can help you work smarter instead of harder  \n",
       "4       I can help you work smarter instead of harder  \n",
       "5       I can help you work smarter instead of harder  \n",
       "6       I can help you work smarter instead of harder  \n",
       "7       I can help you work smarter instead of harder  \n",
       "8       I can help you work smarter instead of harder  \n",
       "9       I can help you work smarter instead of harder  \n",
       "10      I can help you work smarter instead of harder  \n",
       "11      I can help you work smarter instead of harder  \n",
       "12      I can help you work smarter instead of harder  \n",
       "13      I can help you work smarter instead of harder  \n",
       "14      I can help you work smarter instead of harder  \n",
       "15      I can help you work smarter instead of harder  \n",
       "16      I can help you work smarter instead of harder  \n",
       "17      I can help you work smarter instead of harder  \n",
       "18      I can help you work smarter instead of harder  \n",
       "19  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "20  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "21  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "22  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "23  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "24  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "25  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "26  I'm a relatively new bot, but I'm wise beyond ...  \n",
       "27                     Sorry to come across that way.  \n",
       "28                         Sorry to make you feel so.  \n",
       "29                         Sorry to make you feel so.  \n",
       "30                         Sorry to make you feel so.  \n",
       "31                         Sorry to make you feel so.  \n",
       "32                         Sorry to make you feel so.  \n",
       "33                         Sorry to make you feel so.  \n",
       "34                         Sorry to make you feel so.  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ffill(axis=0, inplace=True)\n",
    "df.head(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43faa049-3c40-4355-91d7-854186bf8813",
   "metadata": {},
   "source": [
    "### Define a function to clean our text for vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8250d460-f532-4a8d-a579-8e8113472435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "    text = str(text).lower() # Convert text to lowercase\n",
    "    special_char_rm = re.sub(r'[^ a-z]', '', text) # Remove special characters\n",
    "    tokens = nltk.word_tokenize(special_char_rm) # Tokenize text\n",
    "    lema = wordnet.WordNetLemmatizer() # Initialize Lemmatization\n",
    "    tag_list = pos_tag(tokens, tagset=None) # POS tagging\n",
    "    lema_words = [] # Initialize empty list\n",
    "    for token, pos_token in tag_list: # I don't know why I've done this\n",
    "        if pos_token.startswith('V'): # Verb\n",
    "            pos_val = 'v'\n",
    "        elif pos_token.startswith('J'): # Adjective\n",
    "            pos_val = 'a'\n",
    "        elif pos_token.startswith('R'): # Adverb\n",
    "            pos_val = 'r'\n",
    "        else:\n",
    "            pos_val = 'n' # Noun\n",
    "        lema_words.append(lema.lemmatize(token, pos_val)) # Perform Lemmatization and append to list\n",
    "    \n",
    "    return \" \".join(lema_words) # Returns lemmatized tokens as a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e56ad-cece-4af1-9cab-e6e216a96890",
   "metadata": {},
   "source": [
    "### Apply the text_normalization function to the 'Context' column, and append the returned data to a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b5bd0e-8348-47c9-aa7e-631e84c6eef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Text Response</th>\n",
       "      <th>Lemmatized Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me about your personality</td>\n",
       "      <td>Just think of me as the ace up your sleeve.</td>\n",
       "      <td>tell me about your personality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want to know you better</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>i want to know you good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Define yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>define yourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>describe yourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell me about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>tell me about yourself</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Context  \\\n",
       "0  Tell me about your personality   \n",
       "1       I want to know you better   \n",
       "2                 Define yourself   \n",
       "3               Describe yourself   \n",
       "4          tell me about yourself   \n",
       "\n",
       "                                   Text Response  \\\n",
       "0    Just think of me as the ace up your sleeve.   \n",
       "1  I can help you work smarter instead of harder   \n",
       "2  I can help you work smarter instead of harder   \n",
       "3  I can help you work smarter instead of harder   \n",
       "4  I can help you work smarter instead of harder   \n",
       "\n",
       "               Lemmatized Context  \n",
       "0  tell me about your personality  \n",
       "1         i want to know you good  \n",
       "2                 define yourself  \n",
       "3               describe yourself  \n",
       "4          tell me about yourself  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Lemmatized Context'] = df['Context'].apply(text_normalization)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff58f0-62e6-4ced-8daf-c6ab657e9e81",
   "metadata": {},
   "source": [
    "### Initialize Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "701e4247-2c66-42ed-ab8f-01cb8230afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b055353-f882-415b-8c16-7657c052b440",
   "metadata": {},
   "source": [
    "### Initialize stopwords var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdd5e18-d54e-4a30-8653-b05bc3538dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de089f2-0ff3-477f-9d97-dc9d21cd0e67",
   "metadata": {},
   "source": [
    "### Fit our 'Lemmatized Context' data to the tf-idf vectorizer, and then fit it to a pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0115e702-2ded-4ec1-9835-bd3ad5dd9911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abort</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>actually</th>\n",
       "      <th>adore</th>\n",
       "      <th>advice</th>\n",
       "      <th>advise</th>\n",
       "      <th>affirmative</th>\n",
       "      <th>afraid</th>\n",
       "      <th>...</th>\n",
       "      <th>yeh</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.641790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.641790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1592 rows × 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abort     about  absolutely  abysmal  actually  adore  advice  advise  \\\n",
       "0       0.0  0.407572         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "1       0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "2       0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "3       0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "4       0.0  0.453790         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "...     ...       ...         ...      ...       ...    ...     ...     ...   \n",
       "1587    0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "1588    0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "1589    0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "1590    0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "1591    0.0  0.000000         0.0      0.0       0.0    0.0     0.0     0.0   \n",
       "\n",
       "      affirmative  afraid  ...  yeh  yep  yes  yet       you      your  youre  \\\n",
       "0             0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.330555    0.0   \n",
       "1             0.0     0.0  ...  0.0  0.0  0.0  0.0  0.218768  0.000000    0.0   \n",
       "2             0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.000000    0.0   \n",
       "3             0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.000000    0.0   \n",
       "4             0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.000000    0.0   \n",
       "...           ...     ...  ...  ...  ...  ...  ...       ...       ...    ...   \n",
       "1587          0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.000000    0.0   \n",
       "1588          0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.000000    0.0   \n",
       "1589          0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.000000    0.0   \n",
       "1590          0.0     0.0  ...  0.0  0.0  0.0  0.0  0.131753  0.000000    0.0   \n",
       "1591          0.0     0.0  ...  0.0  0.0  0.0  0.0  0.000000  0.000000    0.0   \n",
       "\n",
       "      yours  yourself  yup  \n",
       "0       0.0  0.000000  0.0  \n",
       "1       0.0  0.000000  0.0  \n",
       "2       0.0  0.641790  0.0  \n",
       "3       0.0  0.641790  0.0  \n",
       "4       0.0  0.608937  0.0  \n",
       "...     ...       ...  ...  \n",
       "1587    0.0  0.000000  0.0  \n",
       "1588    0.0  0.000000  0.0  \n",
       "1589    0.0  0.000000  0.0  \n",
       "1590    0.0  0.000000  0.0  \n",
       "1591    0.0  0.000000  0.0  \n",
       "\n",
       "[1592 rows x 505 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_tfidf = tfidf.fit_transform(df['Lemmatized Context']).toarray()\n",
    "df_tfidf = pd.DataFrame(lemma_tfidf, columns=tfidf.get_feature_names_out())\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b10b6-74e6-4922-aa6f-15d112c205fd",
   "metadata": {},
   "source": [
    "### List of questions to pass to our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94671c23-2282-4d7a-9832-b585ba0049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'Will you help me, and tell me about yourself?',\n",
    "    'How are you doing?',\n",
    "    'I love you',\n",
    "    'Thanks for the support!',\n",
    "    'Will you reply accurately?',\n",
    "    'Will you marry me?',\n",
    "    'You are amazing, I hope to see you soon!',\n",
    "    'What is the meaning of life?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8d3e1-7138-4d76-9e49-bc6cf680a440",
   "metadata": {},
   "source": [
    "### Function to process the quesiton using TF-idf and return a response from our DF using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a2cd0a-e378-4677-957e-697067608711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_tfidf(text):\n",
    "    question_clean = []\n",
    "    for word in text.split():\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", '', word) # Removes special characters\n",
    "        if word in stopwords: # Remove stopwords from our text\n",
    "            pass\n",
    "        else:\n",
    "            question_clean.append(word)\n",
    "    lemma = text_normalization(\" \".join(question_clean)) # Join and normalize the text\n",
    "    print(f\"Question: {text}\\nLemma: {lemma}\")\n",
    "    lemma_tfidf = tfidf.transform([lemma]).toarray() # apply tf-idf\n",
    "    cos = 1-pairwise_distances(df_tfidf, lemma_tfidf, metric='cosine') # cosine similarity\n",
    "    similarity_index = cos.argmax() # Get index value of highest similarity\n",
    "    response = df['Text Response'].loc[similarity_index]\n",
    "    return print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9bb201-e700-4cfd-9812-5740e9459baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Will you help me, and tell me about yourself?\n",
      "Lemma: will help tell\n",
      "Answer: I'm glad to help. What can I do for you?\n",
      "\n",
      "Question: How are you doing?\n",
      "Lemma: how\n",
      "Answer: Lovely, thanks.\n",
      "\n",
      "Question: I love you\n",
      "Lemma: i love\n",
      "Answer: That's great to hear.\n",
      "\n",
      "Question: Thanks for the support!\n",
      "Lemma: thanks support\n",
      "Answer: It's my pleasure to help.\n",
      "\n",
      "Question: Will you reply accurately?\n",
      "Lemma: will reply accurately\n",
      "Answer: Oh, don't give up on me!\n",
      "\n",
      "Question: Will you marry me?\n",
      "Lemma: will marry\n",
      "Answer: In the virtual sense that I can, sure.\n",
      "\n",
      "Question: You are amazing, I hope to see you soon!\n",
      "Lemma: you amaze i hope see soon\n",
      "Answer: Bye.\n",
      "\n",
      "Question: What is the meaning of life?\n",
      "Lemma: what mean life\n",
      "Answer: Sorry. I think I may have been a little confused by what you said.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    chat_tfidf(question)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb8964-ef52-4f72-aaea-3420e708fafd",
   "metadata": {},
   "source": [
    "### Import the Lancaster Stemmer to prevent duplicates in our texts, this will be used on the training data and on the testing/input data too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63b2104e-c2bd-4ab8-bf62-54021e056154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624fdc7-d40e-4cea-8ac7-086dfbcca16d",
   "metadata": {},
   "source": [
    "### We load a small dataset containing query patterns tagged by intent. The data is processed and is appended to 6 different lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a74fcb1-cf20-4503-9c93-9fe9c62e5f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi ther', 'greeting'), ('how be you', 'greeting'), ('be anyon ther', 'greeting'), ('hey', 'greeting'), ('hol', 'greeting'), ('hello', 'greeting'), ('good day', 'greeting'), ('bye', 'goodbye'), ('see you lat', 'goodbye'), ('goodby', 'goodbye'), ('nic chat to you bye', 'goodbye'), ('til next tim', 'goodbye'), ('thank', 'thanks'), ('thank you', 'thanks'), ('that help', 'thanks'), ('awesom thank', 'thanks'), ('thank for help me', 'thanks'), ('what tim be it', 'query'), ('what the dat today', 'query'), ('be it rain outsid', 'query'), ('tel me about that', 'query'), ('wil you giv me mor inform', 'query'), ('i hav a quest', 'query'), ('that not what i ask', 'angry'), ('yo so annoy', 'angry'), ('yo an idiot', 'angry'), ('you be useless', 'angry'), ('yo the best', 'compliment'), ('i lov you', 'compliment'), ('you look beauty', 'compliment'), ('yo incred', 'compliment'), ('that amaz', 'compliment')]\n"
     ]
    }
   ],
   "source": [
    "intents =  {\n",
    "    \"intents\": [\n",
    "        {\"tag\": \"greeting\",\n",
    "         \"patterns\": [\"Hi there\", \"How are you\", \"Is anyone there?\", \"Hey\", \"Hola\", \"Hello\", \"Good day\"]\n",
    "        },\n",
    "        {\"tag\": \"goodbye\",\n",
    "         \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\", \"Nice chatting to you, bye\", \"Till next time\"]\n",
    "        },\n",
    "        {\"tag\": \"thanks\",\n",
    "         \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\", \"Awesome, thanks\", \"Thanks for helping me\"]\n",
    "        },\n",
    "        {\"tag\": \"query\",\n",
    "         \"patterns\": [\"What time is it?\", \"What's the date today?\", \"Is it raining outside?\", \"Tell me about that\", \"Will you give me more information?\", \"I have a question\"]\n",
    "        },\n",
    "        {\"tag\": \"angry\",\n",
    "         \"patterns\": [\"That's not what I asked!\", \"You're so annoying\", \"You're an idiot\", \"You are useless!\"]\n",
    "        },\n",
    "        {\"tag\": \"compliment\",\n",
    "         \"patterns\": [\"You're the best!\", \"I love you\", \"You look beautiful\", \"You're incredible\", \"That's amazing\"]\n",
    "        } \n",
    "    ]\n",
    " }\n",
    "\n",
    "\n",
    "words = [] # List of unique words in our database: [word1, word2, word3]\n",
    "labels = [] # List of intent tags: [tag1, tag2, tag3]\n",
    "x_docs = [] # List of lists containing tokenized queries: [[query1], [query2], [query3]]\n",
    "x_docs_join = [] # List of lemmatized queries: [query1, query2, query3]\n",
    "x_docs_tagged = [] # List of tuples: [(query1, tag1), (query2, tag2), (query3, tag3)]\n",
    "y_docs = [] # List of tags corresponding to x_docs in order: y_docs[1] == x_docs[1]\n",
    "\n",
    "for intent in intents.values():\n",
    "    for tags in intent:\n",
    "        for pattern in tags['patterns']:\n",
    "            pattern = text_normalization(pattern)\n",
    "            lemma = nltk.word_tokenize(pattern) # Tokenize the query\n",
    "            words.extend(lemma) # Add words to our words list\n",
    "            lemma = [stemmer.stem(w) for w in lemma] # Stemming\n",
    "            x_docs.append(lemma)\n",
    "            x_docs_join.append(\" \".join(lemma))\n",
    "            y_docs.append(tags['tag'])\n",
    "            if tags['tag'] not in labels: # Only append intent tag if not already in the list\n",
    "                labels.append(tags['tag'])\n",
    "            x_docs_tagged.append((\" \".join(lemma), tags['tag']))\n",
    "words = [stemmer.stem(w.lower()) for w in words if w.isalpha()]\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(labels)\n",
    "print(x_docs_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d220fdfd-3aca-4fb7-8b41-ef0f02f09763",
   "metadata": {},
   "source": [
    "### Create a Pandas dataframe and then us Tf-idf to convert it into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364500ba-d8cf-4728-9d5e-d6339fef5744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>amaz</th>\n",
       "      <th>an</th>\n",
       "      <th>annoy</th>\n",
       "      <th>anyon</th>\n",
       "      <th>ask</th>\n",
       "      <th>awesom</th>\n",
       "      <th>be</th>\n",
       "      <th>beauty</th>\n",
       "      <th>best</th>\n",
       "      <th>...</th>\n",
       "      <th>ther</th>\n",
       "      <th>til</th>\n",
       "      <th>tim</th>\n",
       "      <th>to</th>\n",
       "      <th>today</th>\n",
       "      <th>useless</th>\n",
       "      <th>what</th>\n",
       "      <th>wil</th>\n",
       "      <th>yo</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.658825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490077</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597812</td>\n",
       "      <td>0.534081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.796514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.53707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.555133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472933</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472933</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.731038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.604621</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.796514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       about      amaz        an     annoy     anyon       ask    awesom  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.658825  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.796514   \n",
       "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.555133  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "22  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.000000  0.000000  0.000000  0.000000  0.555133  0.000000   \n",
       "24  0.000000  0.000000  0.000000  0.623031  0.000000  0.000000  0.000000   \n",
       "25  0.000000  0.000000  0.623031  0.000000  0.000000  0.000000  0.000000   \n",
       "26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "31  0.000000  0.796514  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          be    beauty      best  ...      ther       til       tim        to  \\\n",
       "0   0.000000  0.000000  0.000000  ...  0.666238  0.000000  0.000000  0.000000   \n",
       "1   0.519876  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.468521  0.000000  0.000000  ...  0.588589  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.490077   \n",
       "11  0.000000  0.000000  0.000000  ...  0.000000  0.597812  0.534081  0.000000   \n",
       "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "17  0.427229  0.000000  0.000000  ...  0.000000  0.000000  0.536715  0.000000   \n",
       "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.391244  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "22  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "24  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "25  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "26  0.519876  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "27  0.000000  0.000000  0.648973  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.650192  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "30  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "31  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      today   useless      what       wil        yo       you  \n",
       "0   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.00000  0.000000  0.000000  0.000000  0.000000  0.441942  \n",
       "2   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8   0.00000  0.000000  0.000000  0.000000  0.000000  0.393067  \n",
       "9   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "10  0.00000  0.000000  0.000000  0.000000  0.000000  0.296271  \n",
       "11  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "12  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "13  0.00000  0.000000  0.000000  0.000000  0.000000  0.622980  \n",
       "14  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "15  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "16  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "17  0.00000  0.000000  0.491274  0.000000  0.000000  0.000000  \n",
       "18  0.53707  0.000000  0.439191  0.000000  0.000000  0.000000  \n",
       "19  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "20  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "21  0.00000  0.000000  0.000000  0.445692  0.000000  0.269439  \n",
       "22  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "23  0.00000  0.000000  0.453963  0.000000  0.000000  0.000000  \n",
       "24  0.00000  0.000000  0.000000  0.000000  0.472933  0.000000  \n",
       "25  0.00000  0.000000  0.000000  0.000000  0.472933  0.000000  \n",
       "26  0.00000  0.731038  0.000000  0.000000  0.000000  0.441942  \n",
       "27  0.00000  0.000000  0.000000  0.000000  0.492625  0.000000  \n",
       "28  0.00000  0.000000  0.000000  0.000000  0.000000  0.517350  \n",
       "29  0.00000  0.000000  0.000000  0.000000  0.000000  0.393067  \n",
       "30  0.00000  0.000000  0.000000  0.000000  0.604621  0.000000  \n",
       "31  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[32 rows x 56 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict = {\n",
    "    'Query': x_docs_join,\n",
    "    'Tag': y_docs\n",
    "}\n",
    "doc_df = pd.DataFrame(doc_dict)\n",
    "doc_lemma_tfidf = tfidf.fit_transform(doc_df['Query']).toarray()\n",
    "doc_tfidf = pd.DataFrame(doc_lemma_tfidf, columns=tfidf.get_feature_names_out())\n",
    "doc_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b210d-27fc-44f7-bd74-0d8a29d2a1d4",
   "metadata": {},
   "source": [
    "### A function to compare our questions against the dataset above using cosine similarity to predict the intent of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c1196e2-0acb-49ac-ac38-697779a49ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_tag(text):\n",
    "    question_clean = []\n",
    "    for word in text.split():\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", '', word) # Removes special characters\n",
    "        if word in stopwords: # Remove stopwords from our text\n",
    "            pass\n",
    "        else:\n",
    "            question_clean.append(word)\n",
    "    lemma = text_normalization(\" \".join(question_clean)) # Join and normalize the text\n",
    "    print(f\"Question: {text}\\nLemma: {lemma}\")\n",
    "    lemma_tfidf = tfidf.transform([lemma]).toarray() # apply tf-idf\n",
    "    cos = 1-pairwise_distances(doc_lemma_tfidf, lemma_tfidf, metric='cosine') # cosine similarity\n",
    "    similarity_index = cos.argmax() # Get index value of highest similarity\n",
    "    response = doc_df['Tag'].loc[similarity_index]\n",
    "    return print(f\"Intent: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d641474a-7563-43b8-9552-f56832736b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Will you help me, and tell me about yourself?\n",
      "Lemma: will help tell\n",
      "Intent: thanks\n",
      "\n",
      "Question: How are you doing?\n",
      "Lemma: how\n",
      "Intent: greeting\n",
      "\n",
      "Question: I love you\n",
      "Lemma: i love\n",
      "Intent: greeting\n",
      "\n",
      "Question: Thanks for the support!\n",
      "Lemma: thanks support\n",
      "Intent: greeting\n",
      "\n",
      "Question: Will you reply accurately?\n",
      "Lemma: will reply accurately\n",
      "Intent: greeting\n",
      "\n",
      "Question: Will you marry me?\n",
      "Lemma: will marry\n",
      "Intent: greeting\n",
      "\n",
      "Question: You are amazing, I hope to see you soon!\n",
      "Lemma: you amaze i hope see soon\n",
      "Intent: goodbye\n",
      "\n",
      "Question: What is the meaning of life?\n",
      "Lemma: what mean life\n",
      "Intent: query\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    chat_tag(question)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab93fa-d808-4429-8fa3-13ef726bd736",
   "metadata": {},
   "source": [
    "### The text has to be converted to a numerical form for the NN to process, we'll be using a BOW approach for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f27afa32-9495-4c01-b92a-ca3cdd98e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "# One hot encoding, Converting the words to numerals\n",
    "for x, doc in enumerate(x_docs):\n",
    "    bag = []\n",
    "    for w in words:\n",
    "        if w in doc:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(y_docs[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda356fa-0a21-4b52-90db-92990f57e0db",
   "metadata": {},
   "source": [
    "### The first layer will be our input layer, our training data defined above will be our single parameter\n",
    "\n",
    "### There are three hidden layers responsible for processing the input data, then there is of course the output layer\n",
    "\n",
    "### The model is saved locally using the tflearn module"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4265ea6-b29a-439b-9260-439aabb3df59",
   "metadata": {},
   "source": [
    "import tflearn\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(training, output, n_epoch=500, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19cc118-4cdc-4214-b3b6-303d6d433600",
   "metadata": {},
   "source": [
    "### Load the model we trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f19afe2-2f2d-4a50-b147-6fada4cfef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aaron\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From c:\\users\\aaron\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tflearn\\initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\aaron\\Jupyter\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "model.load('model.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bfae0b-41b7-4521-81dd-065ea4c8ab6e",
   "metadata": {},
   "source": [
    "### Since our model was trained with a BOW approach, our input must also be converted using BOW for it to work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b968016d-74ce-45d0-a2fd-afb7ca7d8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(inp, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    # inp_words = nltk.word_tokenize(inp)\n",
    "    # inp_words = [stemmer.stem(word.lower()) for word in inp_words]\n",
    "\n",
    "    for inp_word in inp:\n",
    "        for index, word in enumerate(words):\n",
    "            if word == inp_word:\n",
    "                bag[index] = 1\n",
    "\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb4b18-8191-4219-93b5-e24392027ea0",
   "metadata": {},
   "source": [
    "### Function to predict the tag using our model using our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e960eb-38a5-43c6-bdc5-f9a08a9eeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_tag_nn(question):\n",
    "    question_norm = text_normalization(question)\n",
    "    question_lemma = nltk.word_tokenize(question_norm)\n",
    "    question_lemma = [stemmer.stem(word.lower()) for word in question_lemma]\n",
    "    \n",
    "    results = model.predict([bag_of_words(question_lemma, words)])\n",
    "    \n",
    "    results_index = np.argmax(results)\n",
    "\n",
    "    tag = labels[results_index]\n",
    "\n",
    "    for value in intents.values():\n",
    "        for tg in value:\n",
    "            if tg['tag'] == tag:\n",
    "                print(f\"Question: {question}\\nQuestion Stemmed: {question_lemma}\\nTag: {tag}\\nResponse: Null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8522cec9-6460-4311-8cd2-1a3edaf60c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Will you help me, and tell me about yourself?\n",
      "Question Stemmed: ['wil', 'you', 'help', 'me', 'and', 'tel', 'me', 'about', 'yourself']\n",
      "Tag: query\n",
      "Response: Null\n",
      "\n",
      "Question: How are you doing?\n",
      "Question Stemmed: ['how', 'be', 'you', 'do']\n",
      "Tag: greeting\n",
      "Response: Null\n",
      "\n",
      "Question: I love you\n",
      "Question Stemmed: ['i', 'lov', 'you']\n",
      "Tag: compliment\n",
      "Response: Null\n",
      "\n",
      "Question: Thanks for the support!\n",
      "Question Stemmed: ['thank', 'for', 'the', 'support']\n",
      "Tag: thanks\n",
      "Response: Null\n",
      "\n",
      "Question: Will you reply accurately?\n",
      "Question Stemmed: ['wil', 'you', 'reply', 'acc']\n",
      "Tag: compliment\n",
      "Response: Null\n",
      "\n",
      "Question: Will you marry me?\n",
      "Question Stemmed: ['wil', 'you', 'marry', 'me']\n",
      "Tag: greeting\n",
      "Response: Null\n",
      "\n",
      "Question: You are amazing, I hope to see you soon!\n",
      "Question Stemmed: ['you', 'be', 'amaz', 'i', 'hop', 'to', 'see', 'you', 'soon']\n",
      "Tag: compliment\n",
      "Response: Null\n",
      "\n",
      "Question: What is the meaning of life?\n",
      "Question Stemmed: ['what', 'be', 'the', 'mean', 'of', 'lif']\n",
      "Tag: greeting\n",
      "Response: Null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    chat_tag_nn(question)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3a8d6d9-b380-4e3a-8d9b-da233f4d7ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: tell me about big bang\n",
      "Question Stemmed: ['tel', 'me', 'about', 'big', 'bang']\n",
      "Tag: query\n",
      "Response: Null\n"
     ]
    }
   ],
   "source": [
    "chat_tag_nn(\"tell me about big bang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee66393-69ec-40a9-9b43-2dc284f2edb8",
   "metadata": {},
   "source": [
    "### Using cosine similarity we try to predict the intent of the queries in our original large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d328751-3ca3-4999-97b3-c071d7f4289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged_cos = []\n",
    "def df_tag_cos(text):\n",
    "    question_clean = []\n",
    "    for word in text.split():\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", '', word) # Removes special characters\n",
    "        if word in stopwords: # Remove stopwords from our text\n",
    "            pass\n",
    "        else:\n",
    "            question_clean.append(word)\n",
    "    lemma = \" \".join(question_clean) # Join the text\n",
    "    lemma_tfidf = tfidf.transform([lemma]).toarray() # apply tf-idf\n",
    "    cos = 1-pairwise_distances(doc_lemma_tfidf, lemma_tfidf, metric='cosine') # cosine similarity\n",
    "    similarity_index = cos.argmax() # Get index value of highest similarity\n",
    "    tagged = x_docs_tagged[similarity_index][1]\n",
    "    df_tagged_cos.append((text, tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc094200-329e-4f8f-bf8a-32221b8ec4dd",
   "metadata": {},
   "source": [
    "### If we print every 5th result, we see most come up as 'greeting', only one comes up as something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b485c28-abad-4ade-b7ff-7adf80543d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tell me about your personality', 'greeting')\n",
      "('all about you', 'greeting')\n",
      "('who be you', 'greeting')\n",
      "('say about you', 'greeting')\n",
      "('what be your age', 'greeting')\n",
      "('id like to know your age', 'greeting')\n",
      "('youre so annoy', 'angry')\n",
      "('you be irritate', 'greeting')\n",
      "('answer me', 'greeting')\n",
      "('can you answer a question for me', 'greeting')\n",
      "('just answer the question', 'greeting')\n",
      "('answer the question', 'greeting')\n",
      "('give me the answer', 'greeting')\n",
      "('you be horrible', 'greeting')\n",
      "('you be no good', 'greeting')\n",
      "('youre a bad', 'greeting')\n",
      "('youre not very good', 'greeting')\n",
      "('you be bad', 'greeting')\n",
      "('be smarter', 'greeting')\n",
      "('be clever', 'greeting')\n"
     ]
    }
   ],
   "source": [
    "for data in df['Lemmatized Context']:\n",
    "    df_tag_cos(data)\n",
    "for query in df_tagged_cos[0:100:5]:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6f8c7-f89c-464f-a662-b769b203a588",
   "metadata": {},
   "source": [
    "### Now we'll try do the same prediction with our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06669757-6240-4bb7-b07f-f450262c09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged_nn = []\n",
    "\n",
    "def df_tag_nn(text):\n",
    "    text_norm = text_normalization(text)\n",
    "    text_lemma = nltk.word_tokenize(text_norm)\n",
    "    text_lemma = [stemmer.stem(word.lower()) for word in text_lemma]\n",
    "    \n",
    "    results = model.predict([bag_of_words(text_lemma, words)])\n",
    "    \n",
    "    results_index = np.argmax(results)\n",
    "\n",
    "    tag = labels[results_index]\n",
    "    for value in intents.values():\n",
    "        for tg in value:\n",
    "            if tg['tag'] == tag:\n",
    "                df_tagged_nn.append((text, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e720d158-6726-40c4-b1e9-cd7fc9dcb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in df['Lemmatized Context']:\n",
    "    df_tag_nn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4af19e-f712-4afe-9727-b631c2adb126",
   "metadata": {},
   "source": [
    "### We get more varied results with our NN, though not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f84e0e4-d3b1-470e-8385-4549c0881989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tell me about your personality', 'query')\n",
      "('all about you', 'greeting')\n",
      "('who be you', 'compliment')\n",
      "('say about you', 'greeting')\n",
      "('what be your age', 'greeting')\n",
      "('id like to know your age', 'angry')\n",
      "('youre so annoy', 'angry')\n",
      "('you be irritate', 'compliment')\n",
      "('answer me', 'greeting')\n",
      "('can you answer a question for me', 'greeting')\n",
      "('just answer the question', 'greeting')\n",
      "('answer the question', 'greeting')\n",
      "('give me the answer', 'query')\n",
      "('you be horrible', 'compliment')\n",
      "('you be no good', 'greeting')\n",
      "('youre a bad', 'greeting')\n",
      "('youre not very good', 'angry')\n",
      "('you be bad', 'compliment')\n",
      "('be smarter', 'greeting')\n",
      "('be clever', 'greeting')\n"
     ]
    }
   ],
   "source": [
    "for query in df_tagged_nn[0:100:5]:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881262c0-2501-48a6-92e4-7dc9cba70106",
   "metadata": {},
   "source": [
    "### We'll put it back into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c404ec0e-72ad-4934-9822-6f2a69290aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged_nn_query = []\n",
    "df_tagged_nn_tags = []\n",
    "for query, tag in df_tagged_nn:\n",
    "    df_tagged_nn_query.append(query)\n",
    "    df_tagged_nn_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e700f55-5901-413a-95a4-fb40f55f5ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Response</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tell me about your personality</td>\n",
       "      <td>Just think of me as the ace up your sleeve.</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i want to know you good</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>compliment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>define yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>describe yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell me about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tell me some stuff about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>talk some stuff about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>talk about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Query  \\\n",
       "0  tell me about your personality   \n",
       "1         i want to know you good   \n",
       "2                 define yourself   \n",
       "3               describe yourself   \n",
       "4          tell me about yourself   \n",
       "5                   all about you   \n",
       "6    tell me some stuff about you   \n",
       "7       talk some stuff about you   \n",
       "8             talk about yourself   \n",
       "9                  about yourself   \n",
       "\n",
       "                                        Response         Tag  \n",
       "0    Just think of me as the ace up your sleeve.       query  \n",
       "1  I can help you work smarter instead of harder  compliment  \n",
       "2  I can help you work smarter instead of harder       angry  \n",
       "3  I can help you work smarter instead of harder       angry  \n",
       "4  I can help you work smarter instead of harder       query  \n",
       "5  I can help you work smarter instead of harder    greeting  \n",
       "6  I can help you work smarter instead of harder       query  \n",
       "7  I can help you work smarter instead of harder    greeting  \n",
       "8  I can help you work smarter instead of harder    greeting  \n",
       "9  I can help you work smarter instead of harder    greeting  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tagged_nn_data = {\n",
    "    'Query': df_tagged_nn_query,\n",
    "    'Response': df['Text Response'],\n",
    "    'Tag': df_tagged_nn_tags\n",
    "}\n",
    "df_tagged_nn_df = pd.DataFrame(df_tagged_nn_data)\n",
    "df_tagged_nn_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca019cd2-7211-425d-ad16-db7816e64c2e",
   "metadata": {},
   "source": [
    "### We need to make our data more consistent. I decided since a lot of queries have the same response, these should be grouped together, and then can find which intent occures most often, and assign that intent to all queries in that group\n",
    "\n",
    "#### Note: this method is flawed as we earlier used the df.ffill() method to quickly fill in the blank spaces in our data in the response column. This filled in blanks with the last response available. The problem with this is each \"group\" of queries has 1-2 responses, meaning there is always a group with just one response.\n",
    "\n",
    "#### Looking at the raw data, not every query group has multiple responses, meaning it'll be difficult and time consuming to write a function to correct this, I might do this at some point, but for now I'm just going to leave it.\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7791a6-4803-4e40-9896-971d3ae3e3d3",
   "metadata": {},
   "source": [
    "### This fun function processes our data by grouping the data by response, and picking the intent tag that occures most often and assigns it to that group\n",
    "\n",
    "### The function returns three things, a list of tuples, and two dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6e695cb-01bb-4a57-8090-ca1b52437817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tell me about your personality'], [0, 0, 0, 0, 1, 0]]\n",
      "[['i want to know you good', 'define yourself', 'describe yourself', 'tell me about yourself', 'all about you', 'tell me some stuff about you', 'talk some stuff about you', 'talk about yourself', 'about yourself', 'who be you', 'introduce yourself', 'i want to know more about you', 'what be you', 'what be your personality', 'say about you', 'tell me about you', 'why be you here', 'why be you here'], [3, 4, 0, 8, 3, 0]]\n",
      "[['be you year old', 'what be your age', 'how old be you', 'age of yours', 'how old be your platform', 'tell me your age', 'id like to know your age', 'id like to know your age'], [3, 1, 0, 3, 1, 0]]\n",
      "[['you be annoy'], [0, 1, 0, 0, 0, 0]]\n",
      "[['i find you annoy', 'youre incredibly annoy', 'youre so annoy', 'youre too annoy', 'you be annoy me so much', 'you annoy me', 'you be such annoy', 'you be irritate', 'you be annoy me', 'you be very annoy', 'how annoying you be', 'how annoying you be'], [3, 4, 0, 5, 0, 0]]\n",
      "[['answer me'], [0, 0, 0, 1, 0, 0]]\n",
      "[['i want the answer now', 'answer question', 'just answer my question', 'can you answer my question', 'can you answer a question for me', 'answer', 'i want you to answer my question', 'tell me the answer', 'answer my question', 'just answer the question', 'i have a question', 'answer', 'can you answer me', 'give me an answer', 'answer the question', 'answer it', 'i want you to answer me', 'can you answer', 'give me the answer', 'give me the answer'], [3, 0, 0, 13, 4, 0]]\n",
      "[['youre not help me'], [0, 0, 0, 0, 0, 1]]\n",
      "[['you be terrible', 'youre bad', 'youre really bad', 'you be horrible', 'you be waste', 'youre not a good', 'you be useless', 'you be disgust', 'you be no good', 'youre the bad ever', 'you be so useless', 'youre the bad', 'you be so bad', 'youre a bad', 'youre terrible', 'youre very bad', 'you be very bad', 'youre awful', 'youre not very good', 'youre worthless', 'you be a waste of time', 'you be lame', 'you be totally useless', 'you be bad', 'you be not cool', 'you be not good', 'you be not good'], [11, 10, 0, 6, 0, 0]]\n",
      "[['you must learn'], [1, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# the labels var is where all our tags are stored\n",
    "keys = [] # Query, Response, Tag\n",
    "\n",
    "# Assigns the keys [Query, Response, Tag] to our list defined above\n",
    "for key in df_tagged_nn_data.keys():\n",
    "    keys.append(key)\n",
    "\n",
    "def df_tag_norm(df):\n",
    "    df_tagged_tuples = [] # List of tuples containing: (query, response, predicted tag)\n",
    "    df_responses_numerical_dict = {} # Dictionary assigning each response an int: {response1: 0, response2: 1}\n",
    "    df_query_by_response_dict = {} # Categorizes each query by the response given in the DataFrame: {response1: (query1, tag1), (query2, tag2)... response2: etc.}\n",
    "\n",
    "    # Converts our responses to numerical values in the df_responses_numerical_dict dictionary\n",
    "    for value in range(len(df[keys[0]])):\n",
    "        df_tagged_tuples.append((df[keys[0]][value], df[keys[1]][value], df[keys[2]][value]))\n",
    "        if df[keys[1]][value] not in df_responses_numerical_dict:\n",
    "            df_responses_numerical_dict[df[keys[1]][value]] = len(df_responses_numerical_dict)\n",
    "\n",
    "    # Assigns data to the df_query_by_response_dict dictionary, and counts the occurences of different intents and adds them together\n",
    "    # with this we'll be able to identify which tags occures most often, and then we'll assign this tag to all of the queries in\n",
    "    # the groups we created\n",
    "    for df_tuple in df_tagged_tuples:\n",
    "        index = df_responses_numerical_dict[df_tuple[1]]\n",
    "        tags = []\n",
    "        if df_responses_numerical_dict[df_tuple[1]] not in df_query_by_response_dict.keys():\n",
    "            df_query_by_response_dict[index] = [[df_tuple[0]]]\n",
    "        else:\n",
    "            df_query_by_response_dict[index][0].append(df_tuple[0])\n",
    "\n",
    "        # Creates a list to append to our dictionary if it doesn't already exist in the following format: [0, 0, 0, 0, 0, 0]\n",
    "        for tag in labels:\n",
    "            if df_tuple[2] == tag:\n",
    "                tags.append(1)\n",
    "            else:\n",
    "                tags.append(0)\n",
    "\n",
    "        # If it does already exists, we take the values already there, and add them together with the new ones, assigning this new value\n",
    "        # in place of the old one\n",
    "        if len(df_query_by_response_dict[index]) < 2:\n",
    "            df_query_by_response_dict[index].append(tags)\n",
    "        else:\n",
    "            new_tags = []\n",
    "            for tag_index, value in enumerate(df_query_by_response_dict[index][1]):\n",
    "                new_value = tags[tag_index] + value \n",
    "                new_tags.append(new_value)\n",
    "            df_query_by_response_dict[index][1] = new_tags\n",
    "\n",
    "    return df_tagged_tuples, df_responses_numerical_dict, df_query_by_response_dict\n",
    "   \n",
    "df_tagged_tuples, df_responses_numerical_dict, df_query_by_response_dict = df_tag_norm(df_tagged_nn_data)\n",
    "\n",
    "\n",
    "for index in range(10):\n",
    "    print(df_query_by_response_dict[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330292a-e334-4872-9fd7-40719d2fe57d",
   "metadata": {},
   "source": [
    "### Now using the df_query_by_response_dict dictionary created above we'll identify which intent tag occures most often in a given group, and assign that tag to all queries in that group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a308ff8a-55c3-4d42-9251-9d649a9d8844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tell me about your personality'], 'query']\n",
      "[['i want to know you good', 'define yourself', 'describe yourself', 'tell me about yourself', 'all about you', 'tell me some stuff about you', 'talk some stuff about you', 'talk about yourself', 'about yourself', 'who be you', 'introduce yourself', 'i want to know more about you', 'what be you', 'what be your personality', 'say about you', 'tell me about you', 'why be you here', 'why be you here'], 'greeting']\n",
      "[['be you year old', 'what be your age', 'how old be you', 'age of yours', 'how old be your platform', 'tell me your age', 'id like to know your age', 'id like to know your age'], 'angry']\n",
      "[['you be annoy'], 'compliment']\n",
      "[['i find you annoy', 'youre incredibly annoy', 'youre so annoy', 'youre too annoy', 'you be annoy me so much', 'you annoy me', 'you be such annoy', 'you be irritate', 'you be annoy me', 'you be very annoy', 'how annoying you be', 'how annoying you be'], 'greeting']\n",
      "[['answer me'], 'greeting']\n",
      "[['i want the answer now', 'answer question', 'just answer my question', 'can you answer my question', 'can you answer a question for me', 'answer', 'i want you to answer my question', 'tell me the answer', 'answer my question', 'just answer the question', 'i have a question', 'answer', 'can you answer me', 'give me an answer', 'answer the question', 'answer it', 'i want you to answer me', 'can you answer', 'give me the answer', 'give me the answer'], 'greeting']\n",
      "[['youre not help me'], 'thanks']\n",
      "[['you be terrible', 'youre bad', 'youre really bad', 'you be horrible', 'you be waste', 'youre not a good', 'you be useless', 'you be disgust', 'you be no good', 'youre the bad ever', 'you be so useless', 'youre the bad', 'you be so bad', 'youre a bad', 'youre terrible', 'youre very bad', 'you be very bad', 'youre awful', 'youre not very good', 'youre worthless', 'you be a waste of time', 'you be lame', 'you be totally useless', 'you be bad', 'you be not cool', 'you be not good', 'you be not good'], 'angry']\n",
      "[['you must learn'], 'angry']\n"
     ]
    }
   ],
   "source": [
    "def tag_occurence(df_dict):\n",
    "    df_queries_tagged = df_dict.copy()\n",
    "    for key, value in df_dict.items():\n",
    "        highest_val = (0, 0)\n",
    "        for tag in enumerate(value[1]):\n",
    "            if tag[1] > highest_val[1]:\n",
    "                highest_val = tag\n",
    "        df_queries_tagged[key][1] = labels[highest_val[0]]\n",
    "    return df_queries_tagged\n",
    "\n",
    "df_queries_tagged = tag_occurence(df_query_by_response_dict)\n",
    "\n",
    "for index in range(10):\n",
    "    print(df_queries_tagged[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43caca-03d6-4daf-8d68-87549434ff55",
   "metadata": {},
   "source": [
    "### Now we've processed our data, we need to convert it back into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c163a97-132f-4466-915a-729985662d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Response</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tell me about your personality</td>\n",
       "      <td>Just think of me as the ace up your sleeve.</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i want to know you good</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>define yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>describe yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell me about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tell me some stuff about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>talk some stuff about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>talk about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>who be you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>introduce yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i want to know more about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>what be you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>what be your personality</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>say about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tell me about you</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>why be you here</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>why be you here</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>be you year old</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>what be your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>how old be you</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>age of yours</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>how old be your platform</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tell me your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>id like to know your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>id like to know your age</td>\n",
       "      <td>I'm a relatively new bot, but I'm wise beyond ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>you be annoy</td>\n",
       "      <td>Sorry to come across that way.</td>\n",
       "      <td>compliment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i find you annoy</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>youre incredibly annoy</td>\n",
       "      <td>Sorry to make you feel so.</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Query  \\\n",
       "0   tell me about your personality   \n",
       "1          i want to know you good   \n",
       "2                  define yourself   \n",
       "3                describe yourself   \n",
       "4           tell me about yourself   \n",
       "5                    all about you   \n",
       "6     tell me some stuff about you   \n",
       "7        talk some stuff about you   \n",
       "8              talk about yourself   \n",
       "9                   about yourself   \n",
       "10                      who be you   \n",
       "11              introduce yourself   \n",
       "12   i want to know more about you   \n",
       "13                     what be you   \n",
       "14        what be your personality   \n",
       "15                   say about you   \n",
       "16               tell me about you   \n",
       "17                 why be you here   \n",
       "18                 why be you here   \n",
       "19                 be you year old   \n",
       "20                what be your age   \n",
       "21                  how old be you   \n",
       "22                    age of yours   \n",
       "23        how old be your platform   \n",
       "24                tell me your age   \n",
       "25        id like to know your age   \n",
       "26        id like to know your age   \n",
       "27                    you be annoy   \n",
       "28                i find you annoy   \n",
       "29          youre incredibly annoy   \n",
       "\n",
       "                                             Response         Tag  \n",
       "0         Just think of me as the ace up your sleeve.       query  \n",
       "1       I can help you work smarter instead of harder    greeting  \n",
       "2       I can help you work smarter instead of harder    greeting  \n",
       "3       I can help you work smarter instead of harder    greeting  \n",
       "4       I can help you work smarter instead of harder    greeting  \n",
       "5       I can help you work smarter instead of harder    greeting  \n",
       "6       I can help you work smarter instead of harder    greeting  \n",
       "7       I can help you work smarter instead of harder    greeting  \n",
       "8       I can help you work smarter instead of harder    greeting  \n",
       "9       I can help you work smarter instead of harder    greeting  \n",
       "10      I can help you work smarter instead of harder    greeting  \n",
       "11      I can help you work smarter instead of harder    greeting  \n",
       "12      I can help you work smarter instead of harder    greeting  \n",
       "13      I can help you work smarter instead of harder    greeting  \n",
       "14      I can help you work smarter instead of harder    greeting  \n",
       "15      I can help you work smarter instead of harder    greeting  \n",
       "16      I can help you work smarter instead of harder    greeting  \n",
       "17      I can help you work smarter instead of harder    greeting  \n",
       "18      I can help you work smarter instead of harder    greeting  \n",
       "19  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "20  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "21  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "22  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "23  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "24  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "25  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "26  I'm a relatively new bot, but I'm wise beyond ...       angry  \n",
       "27                     Sorry to come across that way.  compliment  \n",
       "28                         Sorry to make you feel so.    greeting  \n",
       "29                         Sorry to make you feel so.    greeting  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_create(df):\n",
    "    proc_query = []\n",
    "    proc_response = []\n",
    "    proc_tag = []\n",
    "\n",
    "    for key, value in df.items():\n",
    "        for query in value[0]:\n",
    "            proc_query.append(query)\n",
    "            proc_response.append(list(df_responses_numerical_dict.keys())[key])\n",
    "            proc_tag.append(value[1])\n",
    "\n",
    "    data_proc = {\n",
    "        'Query': proc_query,\n",
    "        'Response': proc_response,\n",
    "        'Tag': proc_tag\n",
    "    }\n",
    "    df_proc = pd.DataFrame(data_proc)\n",
    "    return df_proc\n",
    "    \n",
    "df_proc = df_create(df_queries_tagged)\n",
    "df_proc.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd1dcf-35f1-426b-a83e-8055fdc695b8",
   "metadata": {},
   "source": [
    "### Now we've got our database (somewhat) uniformally tagged with predicted intent, we can perform cosine similarity on the users input to try estimate the queries intent with hopefully a greater deal of accuracy than we've previously had, as well as providing a response from our DataFrame.\n",
    "\n",
    "### First we transform and fit our data using Tf-idf, and then run the same function we used earlier for cosine similarity. I've removed the stopwords removal function here as I found it increased accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00b5fa91-905c-43a0-bf06-19f63c5e6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc_transform = tfidf.fit_transform(df_proc['Query']).toarray()\n",
    "df_proc_tfidf = pd.DataFrame(df_proc_transform, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "459dd87f-0412-4af5-9521-23c03a9c27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_tag_optimised(text):\n",
    "    question_clean = []\n",
    "    for word in text.split():\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", '', word) # Removes special characters\n",
    "        # if word in stopwords: # Remove stopwords from our text\n",
    "        #     pass\n",
    "        # else:\n",
    "        question_clean.append(word)\n",
    "    lemma = text_normalization(\" \".join(question_clean)) # Join and normalize the text\n",
    "    print(f\"Question: {text}\\nLemma: {lemma}\")\n",
    "    lemma_tfidf = tfidf.transform([lemma]).toarray() # apply tf-idf\n",
    "    cos = 1-pairwise_distances(df_proc_tfidf, lemma_tfidf, metric='cosine') # cosine similarity\n",
    "    similarity_index = cos.argmax() # Get index value of highest similarity\n",
    "    response = df_proc['Tag'].loc[similarity_index]\n",
    "    print(f\"Similar query: {df_proc['Query'].loc[similarity_index]}\")\n",
    "    return print(f\"Intent: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dc10e8e-b408-4800-b354-4f549aa851ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Will you help me, and tell me about yourself?\n",
      "Lemma: will you help me and tell me about yourself\n",
      "Similar query: tell me about yourself\n",
      "Intent: greeting\n",
      "\n",
      "Question: How are you doing?\n",
      "Lemma: how be you do\n",
      "Similar query: how be you do\n",
      "Intent: greeting\n",
      "\n",
      "Question: I love you\n",
      "Lemma: i love you\n",
      "Similar query: love you\n",
      "Intent: angry\n",
      "\n",
      "Question: Thanks for the support!\n",
      "Lemma: thanks for the support\n",
      "Similar query: thanks for your help\n",
      "Intent: thanks\n",
      "\n",
      "Question: Will you reply accurately?\n",
      "Lemma: will you reply accurately\n",
      "Similar query: i will fire you\n",
      "Intent: angry\n",
      "\n",
      "Question: Will you marry me?\n",
      "Lemma: will you marry me\n",
      "Similar query: marry me\n",
      "Intent: greeting\n",
      "\n",
      "Question: You are amazing, I hope to see you soon!\n",
      "Lemma: you be amaze i hope to see you soon\n",
      "Similar query: see you soon\n",
      "Intent: goodbye\n",
      "\n",
      "Question: What is the meaning of life?\n",
      "Lemma: what be the meaning of life\n",
      "Similar query: how be your life\n",
      "Intent: greeting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    chat_tag_optimised(question)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebac69a-daad-4fec-a08c-04234bca136d",
   "metadata": {},
   "source": [
    "### Considering how small our intent database is, this is a pretty good result, there's very obvious errors, but if I were to take some time to expand the database, perhaps change some of the parameters on the NN training, we could probably get some decent results out of this.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252aecf0-a02d-4848-8fb2-78d40673b1fb",
   "metadata": {},
   "source": [
    "### We're going to try boost the performance of our very small dataset by using EDA_NLP (Easy Data Augmentation NLP): https://github.com/jasonwei20/eda_nlp\n",
    "\n",
    "### We need to format our data to work with EDA_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c82fae79-989d-46bf-966c-3af0b123b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'query', 'query', 'query', 'query', 'query', 'query', 'angry', 'angry', 'angry', 'angry', 'compliment', 'compliment', 'compliment', 'compliment', 'compliment']\n",
      "Converted into:\n",
      "[3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "intent_tags = []\n",
    "intent_tags_numerical = []\n",
    "queries = []\n",
    "\n",
    "for intent in intents.values():\n",
    "    for tags in intent:\n",
    "        for pattern in tags['patterns']:\n",
    "            queries.append(pattern)\n",
    "            intent_tags.append(tags['tag'])\n",
    "  \n",
    "for intent_tag in intent_tags:\n",
    "    for index, label in enumerate(labels):\n",
    "        if label == intent_tag:\n",
    "            intent_tags_numerical.append(index)\n",
    "            \n",
    "print(intent_tags)\n",
    "print(\"Converted into:\")\n",
    "print(intent_tags_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dce4eca-f469-4e17-909d-98f84274db96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Hi there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>How are you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Is anyone there?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>Good day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Bye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>See you later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Goodbye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tag             Query\n",
       "0    3          Hi there\n",
       "1    3       How are you\n",
       "2    3  Is anyone there?\n",
       "3    3               Hey\n",
       "4    3              Hola\n",
       "5    3             Hello\n",
       "6    3          Good day\n",
       "7    2               Bye\n",
       "8    2     See you later\n",
       "9    2           Goodbye"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_dict = {\n",
    "    'Tag': intent_tags_numerical,\n",
    "    'Query': queries    \n",
    "}\n",
    "queries_df = pd.DataFrame(queries_dict)\n",
    "queries_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10879e-54f0-4e44-9a4c-00a9a5fbd03b",
   "metadata": {},
   "source": [
    "### Save our data as a .txt file to be processed by EDA_NLP"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76edc6c3-8adc-4aa2-b8de-067a0b74e37d",
   "metadata": {},
   "source": [
    "np.savetxt(r'queries.txt', queries_df.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a78c4-076a-464e-bd28-6ef69a600672",
   "metadata": {},
   "source": [
    "### Had to run eda_nlp outside of Jupyter"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6976901-3068-41a1-9d66-179f10f2217f",
   "metadata": {},
   "source": [
    "!python eda_nlp/code/augment.py --input='queries.txt' --output='output.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eaee23-62c6-4da1-88e6-8829f104a379",
   "metadata": {},
   "source": [
    "### Since our dataset contains a lot of short sentences, I found that the boosting algorithm I used resulted in a lot of duplicate queries, we'll not add duplicate queries to our lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa8a4303-5d74-4254-9ca3-6d0ccf1eb943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original queries len: 32\n",
      "New queries len: 186\n",
      "[['there', 'hi'], ['hi', 'in', 'that', 'respect', 'there'], ['aloha', 'state', 'there'], ['hi', 'there'], ['in', 'that', 'respect', 'hi', 'there'], ['howdy', 'there'], ['how', 'are', 'you'], ['how', 'represent', 'are', 'you'], ['you', 'are', 'how'], ['are', 'how', 'you'], ['is', 'anyone', 'there'], ['there', 'anyone', 'is'], ['anyone', 'is', 'there'], ['is', 'anyone', 'in', 'that', 'respect', 'there'], ['in', 'that', 'respect', 'is', 'anyone', 'there'], ['is', 'in', 'that', 'respect', 'anyone', 'there'], ['hey'], ['hola'], ['hello'], ['howdy'], ['hi'], ['how', 'do', 'you', 'do'], ['hi', 'hello'], ['salutary', 'day'], ['solar', 'day', 'good', 'day'], ['day', 'good'], ['good', 'day'], ['adept', 'good', 'day'], ['good', 'adept', 'day'], ['good', 'clarence', 'day'], ['au', 'revoir'], ['bye'], ['good', 'bye', 'bye'], ['bye', 'bye'], ['figure', 'you', 'later'], ['you', 'see', 'later'], ['see', 'you', 'get', 'word', 'later'], ['see', 'get', 'word', 'you', 'later'], ['take', 'in', 'you', 'later'], ['see', 'you', 'later'], ['see', 'later', 'you'], ['good', 'bye', 'goodbye'], ['goodbye'], ['good', 'bye'], ['adieu'], ['decent', 'chatting', 'to', 'you', 'bye'], ['nice', 'chatting', 'to', 'you', 'goodbye'], ['nice', 'bye', 'to', 'you', 'chatting'], ['nice', 'chatting', 'to', 'you', 'bye'], ['nice', 'chatting', 'confab', 'to', 'you', 'bye']]\n",
      "\n",
      "Original words len: 58\n",
      "eda_words len: 143\n",
      "['a', 'about', 'adept', 'adieu', 'adjacent', 'aloha', 'amazing', 'ampere', 'an', 'annoying', 'anyone', 'appear', 'appointment', 'are', 'asked', 'associate', 'au', 'avail', 'await', 'awe', 'awesome', 'beautiful', 'best', 'bye', 'calculate', 'chatting', 'clarence', 'clock', 'confab', 'courteous', 'date', 'day', 'decent', 'demand', 'dependable', 'devil', 'dimension', 'distinguish', 'do', 'doubtfulness', 'down', 'enquire', 'figure', 'flummox', 'following', 'for', 'fourth', 'fuck', 'future', 'get']\n"
     ]
    }
   ],
   "source": [
    "eda_words = [] # Unique words\n",
    "eda_queries = [] # x axis\n",
    "eda_queries_join = [] # x axis joined\n",
    "eda_intents = [] # y axis\n",
    "\n",
    "with open('eda_queries.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line_sep = re.search(r\"(\\d*)\\t(.*)\", line) # Group 1 is the tag, group 2 is the query\n",
    "        lemma = nltk.word_tokenize(line_sep.group(2))\n",
    "        if lemma not in eda_queries:\n",
    "            eda_queries.append(lemma)\n",
    "            eda_words.extend(lemma)\n",
    "            eda_queries_join.append(\" \".join(lemma)) # Appends query to our list if it doesn't already exist\n",
    "            eda_intents.append(line_sep.group(1)) # Appends intent tag\n",
    "\n",
    "\n",
    "eda_words = sorted(list(set(eda_words)))\n",
    "            \n",
    "print(f\"Original queries len: {len(queries)}\\nNew queries len: {len(eda_queries_join)}\")\n",
    "print(eda_queries[:50])\n",
    "print(\"\")\n",
    "print(f\"Original words len: {len(words)}\\neda_words len: {len(eda_words)}\")\n",
    "print(eda_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ae6a0-9878-446d-a687-d7a2f7b3d29e",
   "metadata": {},
   "source": [
    "### We generated 320 queries from our 32 query database, once we remove the duplicates we're left with 186\n",
    "\n",
    "### The amount of unique words in our database is also about triple what it was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0008c34-5102-48ea-b8d3-dca11bca3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_intents_alpha = []\n",
    "for intent in eda_intents:\n",
    "    alpha_intent = labels[int(intent)]\n",
    "    eda_intents_alpha.append(alpha_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "773f38d1-c1da-48f1-9c33-e55b8c423068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there hi</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi in that respect there</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aloha state there</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi there</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in that respect hi there</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>howdy there</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how are you</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>how represent are you</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>you are how</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>are how you</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Query       Tag\n",
       "0                  there hi  greeting\n",
       "1  hi in that respect there  greeting\n",
       "2         aloha state there  greeting\n",
       "3                  hi there  greeting\n",
       "4  in that respect hi there  greeting\n",
       "5               howdy there  greeting\n",
       "6               how are you  greeting\n",
       "7     how represent are you  greeting\n",
       "8               you are how  greeting\n",
       "9               are how you  greeting"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_queries_dict = {\n",
    "    'Query': eda_queries_join, \n",
    "    'Tag': eda_intents_alpha\n",
    "}\n",
    "eda_queries_df = pd.DataFrame(eda_queries_dict)\n",
    "eda_queries_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245cebd-c981-4d85-abd5-22a8ac550ec5",
   "metadata": {},
   "source": [
    "### Convert our data to an array for cosine similarity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f610d9bd-a5ea-4778-be54-e0c2014ff9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>adept</th>\n",
       "      <th>adieu</th>\n",
       "      <th>adjacent</th>\n",
       "      <th>aloha</th>\n",
       "      <th>amazing</th>\n",
       "      <th>ampere</th>\n",
       "      <th>an</th>\n",
       "      <th>annoying</th>\n",
       "      <th>anyone</th>\n",
       "      <th>...</th>\n",
       "      <th>unbelievable</th>\n",
       "      <th>useless</th>\n",
       "      <th>what</th>\n",
       "      <th>whats</th>\n",
       "      <th>will</th>\n",
       "      <th>wit</th>\n",
       "      <th>with</th>\n",
       "      <th>word</th>\n",
       "      <th>you</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.777391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.558068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.777391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     about  adept  adieu  adjacent    aloha   amazing  ampere   an  annoying  \\\n",
       "0      0.0    0.0    0.0       0.0  0.00000  0.000000     0.0  0.0       0.0   \n",
       "1      0.0    0.0    0.0       0.0  0.00000  0.000000     0.0  0.0       0.0   \n",
       "2      0.0    0.0    0.0       0.0  0.67365  0.000000     0.0  0.0       0.0   \n",
       "3      0.0    0.0    0.0       0.0  0.00000  0.000000     0.0  0.0       0.0   \n",
       "4      0.0    0.0    0.0       0.0  0.00000  0.000000     0.0  0.0       0.0   \n",
       "..     ...    ...    ...       ...      ...       ...     ...  ...       ...   \n",
       "181    0.0    0.0    0.0       0.0  0.00000  0.000000     0.0  0.0       0.0   \n",
       "182    0.0    0.0    0.0       0.0  0.00000  0.777391     0.0  0.0       0.0   \n",
       "183    0.0    0.0    0.0       0.0  0.00000  0.558068     0.0  0.0       0.0   \n",
       "184    0.0    0.0    0.0       0.0  0.00000  0.777391     0.0  0.0       0.0   \n",
       "185    0.0    0.0    0.0       0.0  0.00000  0.000000     0.0  0.0       0.0   \n",
       "\n",
       "     anyone  ...  unbelievable  useless  what  whats  will  wit  with  word  \\\n",
       "0       0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "1       0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "2       0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "3       0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "4       0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "..      ...  ...           ...      ...   ...    ...   ...  ...   ...   ...   \n",
       "181     0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "182     0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "183     0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "184     0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "185     0.0  ...           0.0      0.0   0.0    0.0   0.0  0.0   0.0   0.0   \n",
       "\n",
       "     you  youre  \n",
       "0    0.0    0.0  \n",
       "1    0.0    0.0  \n",
       "2    0.0    0.0  \n",
       "3    0.0    0.0  \n",
       "4    0.0    0.0  \n",
       "..   ...    ...  \n",
       "181  0.0    0.0  \n",
       "182  0.0    0.0  \n",
       "183  0.0    0.0  \n",
       "184  0.0    0.0  \n",
       "185  0.0    0.0  \n",
       "\n",
       "[186 rows x 141 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_lemma_tfidf = tfidf.fit_transform(eda_queries_df['Query']).toarray()\n",
    "eda_tfidf = pd.DataFrame(eda_lemma_tfidf, columns=tfidf.get_feature_names_out())\n",
    "eda_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5559bed-ceaf-4c38-a0d3-5918949567dd",
   "metadata": {},
   "source": [
    "### We'll again use BOW to convert our data to a numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52375aa5-2dd8-4021-8b96-de7596ad6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_training = []\n",
    "eda_output = []\n",
    "eda_out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "# One hot encoding, Converting the words to numerals\n",
    "for x, doc in enumerate(eda_queries):\n",
    "    bag = []\n",
    "    for w in eda_words:\n",
    "        if w in doc:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(eda_intents_alpha[x])] = 1\n",
    "\n",
    "    eda_training.append(bag)\n",
    "    eda_output.append(output_row)\n",
    "\n",
    "\n",
    "eda_training = np.array(eda_training)\n",
    "eda_output = np.array(eda_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae0154-9a2f-40ac-b73f-b4623e2174e0",
   "metadata": {},
   "source": [
    "### I trained the model initially using the same code as I used earlier, but I found not only did I get better results by removing 'batch_size=8', but it also processed significantly quicker. I suspect that the better results come from the model no longer overfitting the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9010bb51-e894-428c-8d29-5abe12b78ebd",
   "metadata": {},
   "source": [
    "tf.compat.v1.reset_default_graph() # Without this, you cannot train a NN twice in one notebook\n",
    "\n",
    "eda_net = tflearn.input_data(shape=[None, len(eda_training[0])])\n",
    "eda_net = tflearn.fully_connected(eda_net, 10)\n",
    "eda_net = tflearn.fully_connected(eda_net, 10)\n",
    "eda_net = tflearn.fully_connected(eda_net, 10)\n",
    "eda_net = tflearn.fully_connected(eda_net, len(eda_output[0]), activation='softmax')\n",
    "eda_net = tflearn.regression(eda_net)\n",
    "\n",
    "model_eda = tflearn.DNN(eda_net)\n",
    "model_eda.fit(eda_training, eda_output, n_epoch=500, show_metric=True)\n",
    "model_eda.save('model_eda.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f537fca-f80b-419f-b1a2-8bb4e5a4e107",
   "metadata": {},
   "source": [
    "### Load the model we trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c41e94d9-92b8-467e-8c14-b1586b8a643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\aaron\\Jupyter\\model_eda.tflearn\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph() # Without this, you cannot train a NN twice in one notebook\n",
    "\n",
    "eda_net = tflearn.input_data(shape=[None, len(eda_training[0])])\n",
    "eda_net = tflearn.fully_connected(eda_net, 10)\n",
    "eda_net = tflearn.fully_connected(eda_net, 10)\n",
    "eda_net = tflearn.fully_connected(eda_net, 10)\n",
    "eda_net = tflearn.fully_connected(eda_net, len(eda_output[0]), activation='softmax')\n",
    "\n",
    "model_eda = tflearn.DNN(eda_net)\n",
    "model_eda.load('model_eda.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3250c6-6c68-45b0-a36e-e12bce465217",
   "metadata": {},
   "source": [
    "### Now we've trained our new model, we'll test it in the same way we tested our first one earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca74507d-ceb3-487e-83b6-84302f74a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_tag_nn_eda(question):\n",
    "    question_norm = text_normalization(question)\n",
    "    question_lemma = nltk.word_tokenize(question_norm)\n",
    "    # question_lemma = [stemmer.stem(word.lower()) for word in question_lemma]\n",
    "    \n",
    "    results = model_eda.predict([bag_of_words(question_lemma, eda_words)])\n",
    "    \n",
    "    results_index = np.argmax(results)\n",
    "\n",
    "    tag = labels[results_index]\n",
    "\n",
    "    for value in intents.values():\n",
    "        for tg in value:\n",
    "            if tg['tag'] == tag:\n",
    "                print(f\"Question: {question}\\nQuestion Stemmed: {question_lemma}\\nTag: {tag}\\nResponse: Null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b310fc2-feb9-43f4-ad87-58469fdf9e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Will you help me, and tell me about yourself?\n",
      "Question Stemmed: ['will', 'you', 'help', 'me', 'and', 'tell', 'me', 'about', 'yourself']\n",
      "Tag: query\n",
      "Response: Null\n",
      "\n",
      "Question: How are you doing?\n",
      "Question Stemmed: ['how', 'be', 'you', 'do']\n",
      "Tag: greeting\n",
      "Response: Null\n",
      "\n",
      "Question: I love you\n",
      "Question Stemmed: ['i', 'love', 'you']\n",
      "Tag: compliment\n",
      "Response: Null\n",
      "\n",
      "Question: Thanks for the support!\n",
      "Question Stemmed: ['thanks', 'for', 'the', 'support']\n",
      "Tag: thanks\n",
      "Response: Null\n",
      "\n",
      "Question: Will you reply accurately?\n",
      "Question Stemmed: ['will', 'you', 'reply', 'accurately']\n",
      "Tag: query\n",
      "Response: Null\n",
      "\n",
      "Question: Will you marry me?\n",
      "Question Stemmed: ['will', 'you', 'marry', 'me']\n",
      "Tag: query\n",
      "Response: Null\n",
      "\n",
      "Question: You are amazing, I hope to see you soon!\n",
      "Question Stemmed: ['you', 'be', 'amaze', 'i', 'hope', 'to', 'see', 'you', 'soon']\n",
      "Tag: goodbye\n",
      "Response: Null\n",
      "\n",
      "Question: What is the meaning of life?\n",
      "Question Stemmed: ['what', 'be', 'the', 'meaning', 'of', 'life']\n",
      "Tag: query\n",
      "Response: Null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    chat_tag_nn_eda(question)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a026a-eb88-449f-9d02-57a182a629e9",
   "metadata": {},
   "source": [
    "### We'll try predict the intents of our original DataFrame once more with our new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cacfb2d1-ac9f-4fa1-95e1-5e9b9a924784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged_eda_nn = []\n",
    "\n",
    "def df_tag_eda_nn(text):\n",
    "    text_norm = text_normalization(text)\n",
    "    text_lemma = nltk.word_tokenize(text_norm)\n",
    "    # text_lemma = [stemmer.stem(word.lower()) for word in text_lemma]\n",
    "    \n",
    "    results = model_eda.predict([bag_of_words(text_lemma, eda_words)])\n",
    "    \n",
    "    results_index = np.argmax(results)\n",
    "\n",
    "    tag = labels[results_index]\n",
    "    for value in intents.values():\n",
    "        for tg in value:\n",
    "            if tg['tag'] == tag:\n",
    "                df_tagged_eda_nn.append((text, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8dc0407d-e0de-4ceb-a2a0-f61a5a0e92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in df['Lemmatized Context']:\n",
    "    df_tag_eda_nn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fbde43d-b421-4c97-9e07-e5c8ec40b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tell me about your personality', 'query')\n",
      "('all about you', 'greeting')\n",
      "('who be you', 'thanks')\n",
      "('say about you', 'query')\n",
      "('what be your age', 'query')\n",
      "('id like to know your age', 'thanks')\n",
      "('youre so annoy', 'angry')\n",
      "('you be irritate', 'thanks')\n",
      "('answer me', 'query')\n",
      "('can you answer a question for me', 'query')\n",
      "('just answer the question', 'query')\n",
      "('answer the question', 'query')\n",
      "('give me the answer', 'thanks')\n",
      "('you be horrible', 'thanks')\n",
      "('you be no good', 'greeting')\n",
      "('youre a bad', 'greeting')\n",
      "('youre not very good', 'angry')\n",
      "('you be bad', 'thanks')\n",
      "('be smarter', 'thanks')\n",
      "('be clever', 'thanks')\n"
     ]
    }
   ],
   "source": [
    "for query in df_tagged_eda_nn[0:100:5]:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ae35697-0f25-43fe-9898-352e22da3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged_eda_nn_query = []\n",
    "df_tagged_eda_nn_tags = []\n",
    "for query, tag in df_tagged_eda_nn:\n",
    "    df_tagged_eda_nn_query.append(query)\n",
    "    df_tagged_eda_nn_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8be9fce9-d7c9-4e67-8be6-2a9a4bfc113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged_eda_nn_data = {\n",
    "    'Query': df_tagged_eda_nn_query,\n",
    "    'Response': df['Text Response'],\n",
    "    'Tag': df_tagged_eda_nn_tags\n",
    "}\n",
    "df_tagged_eda_nn_df = pd.DataFrame(df_tagged_nn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe3790a3-0a65-49fe-973b-813f97c471c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged_tuples_eda, df_responses_numerical_dict_eda, df_query_by_response_dict_eda = df_tag_norm(df_tagged_eda_nn_df)\n",
    "df_queries_eda_tagged = tag_occurence(df_query_by_response_dict_eda)\n",
    "df_eda_proc = df_create(df_queries_eda_tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443bf1c-5979-4ca6-87a1-df0bd2a042a0",
   "metadata": {},
   "source": [
    "### We'll write our final chat function, to use our newest NN model to check for intent, obtain a response using our original cosine similarity method, and also obtain the predicted intent from our response we obtained via cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f06050d9-7166-4a1d-a86e-519661848b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_tfidf = tfidf.fit_transform(df['Lemmatized Context']).toarray()\n",
    "df_tfidf = pd.DataFrame(lemma_tfidf, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "def chat_nn_eda(question):\n",
    "    question_norm = text_normalization(question)\n",
    "    question_lemma = nltk.word_tokenize(question_norm)\n",
    "    # Predict intent using our NN model\n",
    "    results = model_eda.predict([bag_of_words(question_lemma, eda_words)])\n",
    "    results_index = np.argmax(results)\n",
    "    tag = labels[results_index]\n",
    "\n",
    "    # Obtain response\n",
    "    question_clean = []\n",
    "    for word in question.split():\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", '', word) # Removes special characters\n",
    "        if word in stopwords: # Remove stopwords from our text\n",
    "            pass\n",
    "        else:\n",
    "            question_clean.append(word)\n",
    "    lemma = text_normalization(\" \".join(question_clean)) # Join and normalize the text\n",
    "    response = df['Text Response'].loc[results_index]\n",
    "    lemma_tfidf = tfidf.transform([lemma]).toarray() # apply tf-idf\n",
    "    cos = 1-pairwise_distances(df_tfidf, lemma_tfidf, metric='cosine') # cosine similarity\n",
    "    similarity_index = cos.argmax() # Get index value of highest similarity\n",
    "    response = df['Text Response'].loc[similarity_index]\n",
    "    response_tag = df_eda_proc['Tag'].loc[similarity_index]\n",
    "    \n",
    "    if response_tag == tag:\n",
    "        match = True\n",
    "    else:\n",
    "        match = False\n",
    "\n",
    "\n",
    "    for value in intents.values():\n",
    "        for tg in value:\n",
    "            if tg['tag'] == tag:\n",
    "                print(f\"Question: {question}\\nIntent: {tag}\\nResponse: {response}\\ndf_intent: {response_tag}\\nTag match: {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16269791-d878-4740-9230-91918bd33b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Will you help me, and tell me about yourself?\n",
      "Intent: query\n",
      "Response: I'm glad to help. What can I do for you?\n",
      "df_intent: thanks\n",
      "Tag match: False\n",
      "\n",
      "Question: How are you doing?\n",
      "Intent: greeting\n",
      "Response: Lovely, thanks.\n",
      "df_intent: greeting\n",
      "Tag match: True\n",
      "\n",
      "Question: I love you\n",
      "Intent: compliment\n",
      "Response: That's great to hear.\n",
      "df_intent: angry\n",
      "Tag match: False\n",
      "\n",
      "Question: Thanks for the support!\n",
      "Intent: thanks\n",
      "Response: It's my pleasure to help.\n",
      "df_intent: thanks\n",
      "Tag match: True\n",
      "\n",
      "Question: Will you reply accurately?\n",
      "Intent: query\n",
      "Response: Oh, don't give up on me!\n",
      "df_intent: angry\n",
      "Tag match: False\n",
      "\n",
      "Question: Will you marry me?\n",
      "Intent: query\n",
      "Response: In the virtual sense that I can, sure.\n",
      "df_intent: greeting\n",
      "Tag match: False\n",
      "\n",
      "Question: You are amazing, I hope to see you soon!\n",
      "Intent: goodbye\n",
      "Response: Bye.\n",
      "df_intent: goodbye\n",
      "Tag match: True\n",
      "\n",
      "Question: What is the meaning of life?\n",
      "Intent: query\n",
      "Response: Sorry. I think I may have been a little confused by what you said.\n",
      "df_intent: compliment\n",
      "Tag match: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    chat_nn_eda(question)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07089368-db2d-49ad-8b65-7253ad584965",
   "metadata": {},
   "source": [
    "### Intent prediction on my questions appears decently accurate, however it is of course a tiny test set to use against my NN. The intent prediction for the original dataframe starts to show the short comings of my NN.\n",
    "\n",
    "### However, considering I initially trained this NN on a such a small database, I'm really happy with the results I've achieved. I'm confident that if I were to start expanding the database, adding in more tagged queries, I'd see an increase in accuracy.\n",
    "\n",
    "### I also wonder if I could improve the NN by labelling its predictions as True or False, allowing it to learn from its mistakes. I've read about this being possible with a NN, but I've no idea how I'd implement something like this, so it's something I'll need to research further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3b551-926a-4851-a808-5242fa6a2f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
